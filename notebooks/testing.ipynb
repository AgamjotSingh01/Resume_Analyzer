{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\agamj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\agamj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\agamj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\agamj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Initialize stopwords and lemmatizer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Preprocessing function\n",
    "def clean_text(text):\n",
    "    \"\"\"Cleans and preprocesses text data.\"\"\"\n",
    "    if pd.isnull(text):  # Handle missing values\n",
    "        return \"\"\n",
    "    \n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'\\W', ' ', text)  # Remove special characters\n",
    "    tokens = word_tokenize(text)  # Tokenization means to convert the sentence into tokens\n",
    "    tokens = [word for word in tokens if word not in stop_words]  # Remove stopwords removing words like the is i am\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]  # Lemmatization converting words into its original form eg. running to run\n",
    "    \n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Function to clean dataset\n",
    "def preprocess_dataset(file_path):\n",
    "    \"\"\"Loads dataset, handles missing values, combines text columns, and applies text preprocessing.\"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Drop rows where critical columns are missing\n",
    "    df = df.dropna(subset=['job_id', 'description'])\n",
    "\n",
    "    # Fill categorical missing values\n",
    "    df['company_name'] = df['company_name'].fillna(\"Unknown\")\n",
    "    df['location'] = df['location'].fillna(\"Unknown\")\n",
    "    df['formatted_experience_level'] = df['formatted_experience_level'].fillna(\"Unknown\")\n",
    "    df['skills_desc'] = df['skills_desc'].fillna(\"No skills provided\")\n",
    "\n",
    "    # Fill numerical missing values with median salary\n",
    "    for col in ['max_salary', 'min_salary', 'med_salary', 'normalized_salary']:\n",
    "        df[col] = df[col].fillna(df[col].median())\n",
    "\n",
    "    # Drop unnecessary columns\n",
    "    df = df.drop(columns=['pay_period', 'views', 'company_id', 'applies', 'remote_allowed',\n",
    "                          'job_posting_url', 'application_url', 'expiry', 'closed_time',\n",
    "                          'listed_time', 'posting_domain', 'sponsored'], errors='ignore')\n",
    "\n",
    "    # Combine job description and skills description\n",
    "    df['combined_description'] = df['description'].fillna('') + \" \" + df['skills_desc'].fillna('')\n",
    "\n",
    "    # Apply text preprocessing on the combined text\n",
    "    df['clean_text'] = df['combined_description'].apply(clean_text)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Sample of cleaned data:\n",
      "                                combined_description  \\\n",
      "0  Job descriptionA leading real estate firm in N...   \n",
      "1  At Aspen Therapy and Wellness , we are committ...   \n",
      "2  The National Exemplar is accepting application...   \n",
      "3  Senior Associate Attorney - Elder Law / Trusts...   \n",
      "4  Looking for HVAC service tech with experience ...   \n",
      "\n",
      "                                          clean_text  \n",
      "0  job descriptiona leading real estate firm new ...  \n",
      "1  aspen therapy wellness committed serving clien...  \n",
      "2  national exemplar accepting application assist...  \n",
      "3  senior associate attorney elder law trust esta...  \n",
      "4  looking hvac service tech experience commerica...  \n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Test the function with a sample dataset\n",
    "    df_cleaned = preprocess_dataset('../datasets/job_description.csv')\n",
    "\n",
    "    # Print a sample of cleaned data\n",
    "    print(\"\\n✅ Sample of cleaned data:\")\n",
    "    print(df_cleaned[['combined_description', 'clean_text']].head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
